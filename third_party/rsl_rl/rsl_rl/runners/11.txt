# ✅ 新版 ReplayBuffer：高效、单条经验存储、支持 CPU 存储
import torch
import numpy as np

class ReplayBuffer:
    def __init__(self, buffer_size, obs_shape, action_shape, device="cpu", optimize_memory_usage=False):
        self.buffer_size = buffer_size
        self.device = torch.device(device)
        self.optimize_memory_usage = optimize_memory_usage

        self.observations = torch.zeros((buffer_size, *obs_shape), dtype=torch.float32)
        self.actions = torch.zeros((buffer_size, *action_shape), dtype=torch.float32)
        self.rewards = torch.zeros((buffer_size, 1), dtype=torch.float32)
        self.dones = torch.zeros((buffer_size, 1), dtype=torch.float32)

        if not optimize_memory_usage:
            self.next_observations = torch.zeros((buffer_size, *obs_shape), dtype=torch.float32)

        self.ptr = 0
        self.full = False

    def __len__(self):
        return self.buffer_size if self.full else self.ptr

    def add_transition(self, obs, action, reward, next_obs, done):
        n = obs.shape[0]  # n_envs
        for i in range(n):
            idx = self.ptr
            self.observations[idx] = torch.tensor(obs[i], dtype=torch.float32)
            self.actions[idx] = torch.tensor(action[i], dtype=torch.float32)
            self.rewards[idx] = torch.tensor([[reward[i]]], dtype=torch.float32)
            self.dones[idx] = torch.tensor([[done[i]]], dtype=torch.float32)

            if self.optimize_memory_usage:
                next_idx = (idx + 1) % self.buffer_size
                self.observations[next_idx] = torch.tensor(next_obs[i], dtype=torch.float32)
            else:
                self.next_observations[idx] = torch.tensor(next_obs[i], dtype=torch.float32)

            self.ptr = (self.ptr + 1) % self.buffer_size
            if self.ptr == 0:
                self.full = True

    def sample(self, batch_size):
        max_size = len(self)
        indices = np.random.randint(0, max_size, size=batch_size)

        obs = self.observations[indices].to(self.device)
        actions = self.actions[indices].to(self.device)
        rewards = self.rewards[indices].to(self.device)
        dones = self.dones[indices].to(self.device)

        if self.optimize_memory_usage:
            next_obs = self.observations[(indices + 1) % self.buffer_size].to(self.device)
        else:
            next_obs = self.next_observations[indices].to(self.device)

        return obs, actions, rewards, next_obs, dones

    def is_ready(self, batch_size):
        return len(self) >= batch_size


# ✅ OffPolicyRunner 中相关调用修改（核心改动）
# 1. 初始化时 ReplayBuffer 使用 device="cpu" 且不传 n_envs
self.replay_buffer = ReplayBuffer(
    buffer_size=self.cfg.get("replay_buffer_size", 1_000_000),
    obs_shape=(env.num_obs,),
    action_shape=(env.num_actions,),
    device="cpu"  # 强制存在 CPU
)

# 2. 学习时 sample 后放回 GPU
if len(self.replay_buffer) > self.cfg["batch_size"]:
    batch = self.replay_buffer.sample(self.cfg["batch_size"])
    batch = [b.to(self.device) for b in batch]
    self.alg.update(self.cfg["batch_size"], *batch)  # 假设 update 接收 obs, act, rew, next_obs, done
